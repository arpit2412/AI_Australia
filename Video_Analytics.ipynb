{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "import moviepy.editor as mp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import get_labels\n",
    "from utils.inference import detect_faces\n",
    "from utils.inference import draw_text\n",
    "from utils.inference import draw_bounding_box\n",
    "from utils.inference import apply_offsets\n",
    "from utils.inference import load_detection_model\n",
    "from utils.preprocessor import preprocess_input\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for loading data and images\n",
    "detection_model_path = 'trained_models/detection_models/haarcascade_frontalface_default.xml'\n",
    "emotion_model_path = 'trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5'\n",
    "emotion_labels = get_labels('fer2013')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters for bounding boxes shape\n",
    "frame_window = 10\n",
    "emotion_offsets = (20, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading models\n",
    "face_detection = load_detection_model(detection_model_path)\n",
    "emotion_classifier = load_model(emotion_model_path, compile=False)\n",
    "\n",
    "# getting input model shapes for inference\n",
    "emotion_target_size = emotion_classifier.input_shape[1:3]\n",
    "\n",
    "# starting lists for calculating modes\n",
    "emotion_window = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sentimental analysis of the candidate</h3>\n",
    "\n",
    "\n",
    "<h4>Detected:\n",
    "    <ol>\n",
    "    1. Angry\n",
    "    2. Sad\n",
    "    3. Happy\n",
    "    4. Suprise\n",
    "    5. Other</ol>\n",
    "    </h4>\n",
    "    \n",
    "    \n",
    "Note: Press Q to exit at end.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting video streaming\n",
    "angry = 0\n",
    "sad = 0\n",
    "happy = 0 \n",
    "suprise = 0\n",
    "other = 0\n",
    "cv2.namedWindow('window_frame')\n",
    "video_capture = cv2.VideoCapture('Interview_4.mp4')\n",
    "while True:\n",
    "    bgr_image = video_capture.read()[1]\n",
    "    gray_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)\n",
    "    rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
    "    faces = detect_faces(face_detection, gray_image)\n",
    "\n",
    "    for face_coordinates in faces:\n",
    "\n",
    "        x1, x2, y1, y2 = apply_offsets(face_coordinates, emotion_offsets)\n",
    "        gray_face = gray_image[y1:y2, x1:x2]\n",
    "        try:\n",
    "            gray_face = cv2.resize(gray_face, (emotion_target_size))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        gray_face = preprocess_input(gray_face, True)\n",
    "        gray_face = np.expand_dims(gray_face, 0)\n",
    "        gray_face = np.expand_dims(gray_face, -1)\n",
    "        emotion_prediction = emotion_classifier.predict(gray_face)\n",
    "        emotion_probability = np.max(emotion_prediction)\n",
    "        emotion_label_arg = np.argmax(emotion_prediction)\n",
    "        emotion_text = emotion_labels[emotion_label_arg]\n",
    "        emotion_window.append(emotion_text)\n",
    "\n",
    "        if len(emotion_window) > frame_window:\n",
    "            emotion_window.pop(0)\n",
    "        try:\n",
    "            emotion_mode = mode(emotion_window)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if emotion_text == 'angry':\n",
    "            color = emotion_probability * np.asarray((255, 0, 0))\n",
    "            angry = angry + 1\n",
    "        elif emotion_text == 'sad':\n",
    "            color = emotion_probability * np.asarray((0, 0, 255))\n",
    "            sad = sad + 1\n",
    "        elif emotion_text == 'happy':\n",
    "            color = emotion_probability * np.asarray((255, 255, 0))\n",
    "            happy = happy + 1\n",
    "        elif emotion_text == 'surprise':\n",
    "            color = emotion_probability * np.asarray((0, 255, 255))\n",
    "            suprise = suprise + 1 \n",
    "        else:\n",
    "            color = emotion_probability * np.asarray((0, 255, 0))\n",
    "            other = other + 1\n",
    "\n",
    "        color = color.astype(int)\n",
    "        color = color.tolist()\n",
    "\n",
    "        draw_bounding_box(face_coordinates, rgb_image, color)\n",
    "        draw_text(face_coordinates, rgb_image, emotion_mode,\n",
    "                  color, 0, -45, 1, 1)\n",
    "\n",
    "    bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imshow('window_frame', bgr_image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Convert voice part of the answer and convert to text</h3>\n",
    "\n",
    "\n",
    "\n",
    "<b>Pipeline:</b>\n",
    "\n",
    "1. Convert video to audio : done using MoviePy\n",
    "\n",
    "2. Using Sppech Recognizer convert generated audio to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:  30%|███       | 287/956 [00:00<00:00, 2866.73it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in converted.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "clip = mp.VideoFileClip(r'Interview_4.mp4') \n",
    "clip.audio.write_audiofile(r'converted.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sr.Recognizer()\n",
    "audio = sr.AudioFile(\"converted.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speech_recognition.AudioFile"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello everyone my name is David Burke and I am doing master of data science from the University of Adelaide and I choose this field of study because I've always been interested in this field I have worked for the defence Industries and help them in automation using data science and I feel ready to take my career to the next level so that's why I am currently looking for a new apportionately and recording this video thank you\n"
     ]
    }
   ],
   "source": [
    "with audio as source:\n",
    "    # listen for the data (load audio to memory)\n",
    "    audio_data = r.record(source,duration=120)\n",
    "    # recognize (convert from speech to text)\n",
    "    text = r.recognize_google(audio_data)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Identify most prominent sentiments based on the candidate’s face, e.g. happy, stressed, sad, etc.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#can see using histogram\n",
    "plt.hist([angry,sad,happy,suprise,other])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Identify various nonverbal attributes.</h3>\n",
    "\n",
    "1. Detecing eyeballs and printing motion of eyeball to see if our candidate is confident or not\n",
    "2. Press ESC to exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_cascade = cv2.CascadeClassifier('haarcascade/haarcascade_eye_tree_eyeglasses.xml')\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "cap = cv2.VideoCapture('Interview_4.mp4')\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, img = cap.read()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "     \n",
    "    \n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray, \n",
    "        scaleFactor = 1.1, \n",
    "        minNeighbors = 5,\n",
    "        minSize = (30, 30),\n",
    "        flags = cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = img[y:y+h, x:x+w]\n",
    "        \n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        \n",
    "        pupilFrame = roi_gray\n",
    "        pupilO = roi_gray\n",
    "        windowClose = np.ones((5,5),np.uint8)\n",
    "        windowOpen = np.ones((2,2),np.uint8)\n",
    "        windowErode = np.ones((2,2),np.uint8)\n",
    "        \n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "            cv2.line(roi_color, (ex,ey), ((ex+ew,ey+eh)), (0,0,255),1)\n",
    "            cv2.line(roi_color, (ex+ew,ey), ((ex,ey+eh)), (0,0,255),1)\n",
    "            pupilFrame = cv2.equalizeHist(roi_gray[int(ey+(eh*.25)):(ey+eh), ex:(ex+ew)])\n",
    "            pupilO = pupilFrame\n",
    "            ret, pupilFrame = cv2.threshold(pupilFrame,50,255,cv2.THRESH_BINARY)        #50 ..nothin 70 is better\n",
    "            pupilFrame = cv2.morphologyEx(pupilFrame, cv2.MORPH_CLOSE, windowClose)\n",
    "            pupilFrame = cv2.morphologyEx(pupilFrame, cv2.MORPH_ERODE, windowErode)\n",
    "            pupilFrame = cv2.morphologyEx(pupilFrame, cv2.MORPH_OPEN, windowOpen)\n",
    "\n",
    "            threshold = cv2.inRange(pupilFrame,250,255)        #get the blobs\n",
    "\n",
    "            contours, hierarchy = cv2.findContours(threshold,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            if len(contours) >= 2:\n",
    "                    #find biggest blob\n",
    "                    maxArea = 0\n",
    "                    MAindex = 0            #to get the unwanted frame \n",
    "                    distanceX = []        #delete the left most (for right eye)\n",
    "                    currentIndex = 0 \n",
    "                    for cnt in contours:\n",
    "                        area = cv2.contourArea(cnt)\n",
    "                        center = cv2.moments(cnt)\n",
    "                        if center['m00'] != 0:\n",
    "                            cx = int(center[\"m10\"] / center[\"m00\"])\n",
    "                            cy = int(center[\"m01\"] / center[\"m00\"])\n",
    "                        else:\n",
    "                            cx,cy = 0, 0\n",
    "                        distanceX.append(cx)    \n",
    "                        if area > maxArea:\n",
    "                            maxArea = area\n",
    "                            MAindex = currentIndex\n",
    "                        currentIndex = currentIndex + 1\n",
    "\n",
    "                    del contours[MAindex]        #remove the picture frame contour\n",
    "                    del distanceX[MAindex]\n",
    "\n",
    "            eye = 'right'\n",
    "\n",
    "            if len(contours) >= 2:        #delete the left most blob for right eye\n",
    "                if eye == 'right':\n",
    "                    edgeOfEye = distanceX.index(min(distanceX))\n",
    "                else:\n",
    "                    edgeOfEye = distanceX.index(max(distanceX))    \n",
    "                del contours[edgeOfEye]\n",
    "                del distanceX[edgeOfEye]\n",
    "\n",
    "            if len(contours) >= 1:        #get largest blob\n",
    "                    maxArea = 0\n",
    "                    for cnt in contours:\n",
    "                        area = cv2.contourArea(cnt)\n",
    "                        if area > maxArea:\n",
    "                            maxArea = area\n",
    "                            largeBlob = cnt\n",
    "\n",
    "            if len(largeBlob) > 0:    \n",
    "                    center = cv2.moments(largeBlob)\n",
    "                    cx,cy = int(center['m10']/center['m00']), int(center['m01']/center['m00'])\n",
    "                    cv2.circle(pupilO,(cx,cy),5,255,-1)\n",
    "            \n",
    "            cv2.imshow('Eye',pupilO)\n",
    "            cv2.imshow('Eye1',pupilFrame)\n",
    "            \n",
    "    cv2.imshow('img',img)\n",
    "    k = cv2.waitKey(1) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
